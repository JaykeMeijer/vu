\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{todonotes}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[all]{xy}
\usepackage{listings}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\author{Rik van der Kooij 2526314\\Richard Torenvliet 234234}
\title{Assignment 1 - Design Document}
\begin{document}
\maketitle

%\section{Introduction}


\section{Coarse Grained List}
\subsection{Implementation}
    The coarse grained list is a linked list with one lock to provide a concurrent data structure. All threads will have to acquire this lock before operating the list and unlock it afterwards. This forces only one thread operating on the list at the time. A pseudocode example of the add and remove function looks as follows:
   
\begin{lstlisting}
    add(e) 
        lock()
        insert(e) // insert the element to the list
        unlock()

    remove(e)
        lock()
        rm(e)     // remove the element from the list
        unlock()
\end{lstlisting}

\subsection{Performance Hypothesis}
%   <<<<<<< head
%   let $\#e$ be the number of elements in the data structure, $\#t$ the amount of threads
%   that work with this data structure and $\#w$ the amount of work per thread in
%   comparison to inserting/deleting nodes. by letting one value to stay the same
%   and to other to grow, we can make a hypothesis about the performance of the
%   implementation of the synchronization in combination with the data structure.
%   \begin{itemize}
%   \item $\#e$: if $\#e$ grows and $\#t$ stays equal, only the lookup could influence the performance.
%       i.e. more lookups, more work
%    \item $\#t$: if $\#t$ grows and $\#e$ stays equal, there will be no influence on the performance.
%    \item $\#w$: 
%   \end{itemize}
As only one thread will be able to operate the list at the time other threads will be waiting. Increasing the number of elements in the list will increase the lookup time on the list. If the operating time increases so does the waiting time of all other threads. More elements will therefore decrease the performance of the coarse grained list.

Locking once is cheap. If there are no threads that want to operate on the list simultaneously the performance will be high; one lock operation per add or remove operation. 
With high amount of threads the chance of simultaneous arrival will be high. This results in long waiting time. For high amount of threads the performance will therefore decrease. 

\section{Coarse Grained Tree}
\subsection{Implementation}
%The Course Grained synchronization in a tree also locks the entire
%data structure. By inserting an item the lookup and inserting will have to
%require a lock. Thread Y also needs to wait until
%Thread X is done with these operation for inserting and deleting.

The coarse grained tree is a binary tree which uses one lock to provide a concurrent data structure.

    \begin{lstlisting}
        insert(E e) 
            lock()
            add(E e) // add the element
            unlock()

        remove(E e)
            lock()
            rm(E e)  // remove the element
            unlock()
    \end{lstlisting}

\subsection{Performance Hypothesis}
%\begin{itemize}
% \item $\#e$:   if $\#e$ grows and $\#t$ stays equal, only the lookup could influence the
% performance. \\ i.e. more lookups, more work
% \item $\#t$:   if $\#t$ grows and $\#e$ stays equal, there will be no influence on the performance.
% \item $\#w$:   \todo[inline]{needs to be filled in}
%\end{itemize}

\section{Fine Grained List}
%The concurrent implementation of a linked-list data structure with Fine Grained
%Synchronization means that every item can be locked individually. The
%nodes in the list are locked when read, not the entire data structure. Which
%results in a list where Threads can operate at the same time. To achieve this
%kind of synchronization, the locks have to be two folded. Which is also known 
%as \emph{lock coupling}, Thread X can only acquire the lock for the current 
%Node, if and only if it still has the lock for the previous Node.
\begin{figure}[h]
\centerline{
\xymatrix{
1: &*+++[F=]{head} \ar[r] & *+++[F]{a pred} \ar[r] & *+++[F]{b curr} \\
&locked & \text{request lock} & ..  \\
2: &*+++[F=]{head} \ar[r] & *+++[F=]{a pred} \ar[r] & *+++[F]{b curr} \\
&locked & locked & \text{request lock}
}}
\caption{linked-list coupled lock}
\end{figure}

If thread X locks item 100 in the list, Thread Y can lock and thereby
insert/delete any Node between the start item and the item 100. This
also holds for any other thread. But if thread Y locks the first item in the
list, thread Z will not be able to insert or delete any nodes. So it stalls,
together with all the other Thread that may attempt to access the list.

\subsection{Performance Hypothesis}
If the amount of elements in the data structure is small, a small part of the
list can be accessed by other threads. As the list grows, more items can be 
inserted or deleted simultaneously. Threads block to items at once and block
the road for other threads. But if the list contains more items the threads
will have a greater working area. So the amount of elements that can be inserted/deleted 
per time unit grows.
The hypothesis is that the performance of the list increases when the list 
grows. 

The amount of work performed by a thread to able to insert or delete variates.
It depends on the length of the list and how many lookups have to performed
before deletion. For every lookup, one lock has to performed. This can
influence the performance in a negative way. In worst-case the elements that
are added to the list are already sorted, this means N lookups have to be
performed for a list of N items.

\section{Fine Grained Tree}


\end{document}

